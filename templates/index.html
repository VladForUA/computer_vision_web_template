<html>
  <head>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.0.0/dist/tf.min.js"></script>
    <title>Page Title</title>
  </head>
  <body>
    <p>
      This example shows you the contents of the selected part of your display.
      Click the Start Capture button to begin.
    </p>

    <p>
      <button id="start">Start Capture</button>&nbsp;<button id="stop">
        Stop Capture
      </button>
    </p>
    <div style="position: relative">
      <video id="stream" width="50%" height="50%" autoplay></video>
      <canvas id="detection" style="position: absolute; left: 0"></canvas>
    </div>
  </body>
  </html>

  <script type="module">
    //VARIABLES
    const videoElem = document.getElementById("stream");
    const startElem = document.getElementById("start");
    const stopElem = document.getElementById("stop");
    const CLASSES = [
      "person",
      "bicycle",
      "car",
      "motorcycle",
      "airplane",
      "bus",
      "train",
      "truck",
      "boat",
      "traffic light",
      "fire hydrant",
      "unused",
      "stop sign",
      "parking meter",
      "bench",
      "bird",
      "cat",
      "dog",
      "horse",
      "sheep",
      "cow",
      "elephant",
      "bear",
      "zebra",
      "giraffe",
      "unused",
      "backpack",
      "umbrella",
      "unused",
      "unused",
      "handbag",
      "tie",
      "suitcase",
      "frisbee",
      "skis",
      "snowboard",
      "sports ball",
      "kite",
      "baseball bat",
      "baseball glove",
      "skateboard",
      "surfboard",
      "tennis racket",
      "bottle",
      "unused",
      "wine glass",
      "cup",
      "fork",
      "knife",
      "spoon",
      "bowl",
      "banana",
      "apple",
      "sandwich",
      "orange",
      "broccoli",
      "carrot",
      "hot dog",
      "pizza",
      "donut",
      "cake",
      "chair",
      "couch",
      "potted plant",
      "bed",
      "unused",
      "dining table",
      "unused",
      "unused",
      "toilet",
      "unused",
      "tv",
      "laptop",
      "mouse",
      "remote",
      "keyboard",
      "cell phone",
      "microwave",
      "oven",
      "toaster",
      "sink",
      "refrigerator",
      "unused",
      "book",
      "clock",
      "vase",
      "scissors",
      "teddy bear",
      "hair drier",
      "toothbrush",
    ];
    const displayMediaOptions = {
      video: {
        cursor: "always",
      },
      audio: false,
    };
    const proto = location.protocol.startsWith('https') ? 'wss' : 'ws'
    const wsUri = `${proto}://${location.host}/ws`
    const socket = new WebSocket(wsUri)

    /////////////////////////////////////////////////////


    //FUNCTIONS
    async function startCapture() {
      try {
        videoElem.srcObject = await navigator.mediaDevices.getDisplayMedia(
          displayMediaOptions
        );
      } catch (err) {
        console.error("Error: " + err);
      }
    }

    function stopCapture(evt) {
      let tracks = videoElem.srcObject.getTracks();
      tracks.forEach((track) => track.stop());
      videoElem.srcObject = null;
    }

    async function setupStream(videoRef) {
        if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
          startCapture();

          return new Promise((resolve, _) => {
            videoRef.onloadedmetadata = () => {
              // Prep Canvas
              const detection = document.getElementById('detection')
              const ctx = detection.getContext('2d')
              const imgWidth = videoRef.clientWidth
              const imgHeight = videoRef.clientHeight
              detection.width = imgWidth
              detection.height = imgHeight
              ctx.font = '16px sans-serif'
              ctx.textBaseline = 'top'
              resolve([ctx, imgHeight, imgWidth])
            }
          })
        } else {
          alert('No screen sharing!')
        }
      }

    async function loadModel() {
        await tf.ready()
        const modelPath =
          'https://tfhub.dev/tensorflow/tfjs-model/ssd_mobilenet_v2/1/default/1'

        return await tf.loadGraphModel(modelPath, { fromTFHub: true })
      }

    async function performDetections(model, videoRef, camDetails) {
        const [ctx, imgHeight, imgWidth] = camDetails
        const myTensor = tf.browser.fromPixels(videoRef)
        // SSD Mobilenet single batch
        const readyfied = tf.expandDims(myTensor, 0)
        const results = await model.executeAsync(readyfied)

        // Get a clean tensor of top indices
        const detectionThreshold = 0.4
        const iouThreshold = 0.5
        const maxBoxes = 20
        const prominentDetection = tf.topk(results[0])
        const justBoxes = results[1].squeeze()
        const justValues = prominentDetection.values.squeeze()

        // Move results back to JavaScript in parallel
        const [maxIndices, scores, boxes] = await Promise.all([
          prominentDetection.indices.data(),
          justValues.array(),
          justBoxes.array(),
        ])

        // https://arxiv.org/pdf/1704.04503.pdf, use Async to keep visuals
        const nmsDetections = await tf.image.nonMaxSuppressionWithScoreAsync(
          justBoxes, // [numBoxes, 4]
          justValues, // [numBoxes]
          maxBoxes,
          iouThreshold,
          detectionThreshold,
          1 // 0 is normal NMS, 1 is Soft-NMS for overlapping support
        )

        const chosen = await nmsDetections.selectedIndices.data()
        // Mega Clean
        tf.dispose([
          results[0],
          results[1],
          // model, don't clean this one up for loops
          nmsDetections.selectedIndices,
          nmsDetections.selectedScores,
          prominentDetection.indices,
          prominentDetection.values,
          myTensor,
          readyfied,
          justBoxes,
          justValues,
        ])

        // clear everything each round
        ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height)
        chosen.forEach((detection) => {
          ctx.strokeStyle = '#0F0'
          ctx.lineWidth = 4
          ctx.globalCompositeOperation = 'destination-over'
          const detectedIndex = maxIndices[detection]
          const detectedClass = CLASSES[detectedIndex]
          const detectedScore = scores[detection]
          const dBox = boxes[detection]

          // No negative values for start positions
          const startY = dBox[0] > 0 ? dBox[0] * imgHeight : 0
          const startX = dBox[1] > 0 ? dBox[1] * imgWidth : 0
          const height = (dBox[2] - dBox[0]) * imgHeight
          const width = (dBox[3] - dBox[1]) * imgWidth
          ctx.strokeRect(startX, startY, width, height)
          // Draw the label background.
          ctx.globalCompositeOperation = 'source-over'
          ctx.fillStyle = '#0B0'
          const textHeight = 16
          const textPad = 4
          const label = `${detectedClass} ${Math.round(detectedScore * 100)}%`
          const textWidth = ctx.measureText(label).width
          ctx.fillRect(
            startX,
            startY,
            textWidth + textPad,
            textHeight + textPad
          )
          // Draw the text last to ensure it's on top.
          ctx.fillStyle = '#000000'
          ctx.fillText(label, startX, startY)
        })

        // Loop forever
        requestAnimationFrame(() => {
          performDetections(model, videoRef, camDetails)
        })
      }

    async function startScreenObjectDetection() {
        try {
          const model = await loadModel()
          const stream = document.getElementById('stream')
          const camDetails = await setupStream(stream)
          performDetections(model, stream, camDetails)
        } catch (e) {
          console.error(e)
        }
      }

    /////////////////////////////////////////////////////

    //LISTNERS
    startElem.addEventListener(
      "click",
      function (evt) {
        startScreenObjectDetection();
      },
      false
    );

    stopElem.addEventListener(
      "click",
      function (evt) {
        stopCapture();
      },
      false
    );

    socket.onopen = () => {
        console.log('Connected')
        socket.send("Send message for testing")
      }

    socket.onmessage = (ev) => {
      console.log('Received: ' + ev.data, 'message')
      }

    socket.onclose = () => {
      console.log('Disconnected')
        socket = null
      }

    /////////////////////////////////////////////////////
    //RUN function after start app
    

  </script>

  <style>
    #stream {
      border: 1px solid #999;
      width: 98%;
      max-width: 860px;
    }

    .error {
      color: red;
    }

    .warn {
      color: orange;
    }

    .info {
      color: darkgreen;
    }
  </style>
